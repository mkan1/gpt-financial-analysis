{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "with open('dataset/train.json') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "dataset = np.array([\n",
    "    ('\\n'.join(data['pre_text'] \n",
    "                + data['post_text'] \n",
    "                + [json.dumps(data['table_ori']), json.dumps(data['table'])]) \n",
    "     + '\\n' + data['qa']['question'], \n",
    "     data['qa']['answer'])\n",
    "    for data in data_dict\n",
    "])\n",
    "\n",
    "# Remove questions without an answer\n",
    "clean_dataset = np.array([(p, c) for (p, c) in dataset if c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "with open('formatted_data.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"prompt\", \"completion\"])\n",
    "    writer.writerows(clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 6203 prompt-completion pairs\n",
      "- There are 47 duplicated prompt-completion sets. These are rows: [1511, 1786, 1863, 1899, 1915, 2067, 2360, 2701, 2886, 2995, 3103, 3351, 3372, 3426, 3461, 3493, 3585, 3715, 3719, 3740, 3759, 3773, 3799, 3906, 3908, 3923, 3971, 4035, 4041, 4229, 4332, 4451, 4638, 4700, 4728, 4745, 4912, 4913, 5030, 5175, 5273, 5324, 5367, 5419, 5451, 5484, 5511]\n",
      "- There are 45 examples that are very long. These are rows: [66, 413, 494, 533, 972, 1121, 1281, 1308, 1319, 1391, 1465, 1568, 1573, 1655, 1975, 2137, 2140, 2223, 2519, 2799, 2912, 2923, 2963, 3249, 3282, 3534, 3947, 4018, 4194, 4244, 4270, 4297, 4343, 4413, 4633, 5702, 5704, 5795, 5876, 5929, 5969, 5992, 6023, 6167, 6189]\n",
      "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV into format usable by GPT\n",
    "!openai tools fine_tunes.prepare_data -f \"formatted_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "with open('dataset/test.json') as f:\n",
    "    valid_data_dict = json.load(f)\n",
    "\n",
    "valid_dataset = np.array([\n",
    "    ('\\n'.join(data['pre_text'] \n",
    "                + data['post_text'] \n",
    "                + [json.dumps(data['table_ori']), json.dumps(data['table'])]) \n",
    "     + '\\n' + data['qa']['question'], \n",
    "     data['qa']['answer'])\n",
    "    for data in valid_data_dict\n",
    "])\n",
    "\n",
    "# Remove questions without an answer\n",
    "clean_valid_dataset = np.array([(p, c) for (p, c) in valid_dataset if c])\n",
    "\n",
    "# Save to CSV\n",
    "with open('formatted_valid_data.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"prompt\", \"completion\"])\n",
    "    writer.writerows(clean_valid_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 1133 prompt-completion pairs\n",
      "- There are 9 duplicated prompt-completion sets. These are rows: [222, 264, 535, 662, 755, 760, 951, 953, 957]\n",
      "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
      "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "- [Recommended] Remove 9 duplicate rows [Y/n]: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV into format usable by GPT\n",
    "!openai tools fine_tunes.prepare_data -f \"formatted_valid_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"api_key.txt\", \"r\")\n",
    "api_key = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-mjKGzWqB6QJgbR337S2mQTgf at 0x7fe478362900> JSON: {\n",
       "  \"bytes\": 28792529,\n",
       "  \"created_at\": 1684274350,\n",
       "  \"filename\": \"file\",\n",
       "  \"id\": \"file-mjKGzWqB6QJgbR337S2mQTgf\",\n",
       "  \"object\": \"file\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload training data to OpenAI\n",
    "training_upload_response = openai.File.create(\n",
    "  file=open(\"formatted_data_prepared.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "training_file_id = training_upload_response.id\n",
    "training_upload_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-wJevgYfAUGFx50S5xR4Y9Od8 at 0x7fe4f8459c20> JSON: {\n",
       "  \"bytes\": 5219225,\n",
       "  \"created_at\": 1684274410,\n",
       "  \"filename\": \"file\",\n",
       "  \"id\": \"file-wJevgYfAUGFx50S5xR4Y9Od8\",\n",
       "  \"object\": \"file\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_upload_response = openai.File.create(\n",
    "  file=open(\"formatted_valid_data_prepared.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "valid_file_id = valid_upload_response.id\n",
    "valid_upload_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-PzwpGKewlzXC1fDppl3IAiin at 0x7fe4bb475180> JSON: {\n",
       "  \"created_at\": 1684274446,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1684274446,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-PzwpGKewlzXC1fDppl3IAiin\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": null,\n",
       "    \"learning_rate_multiplier\": null,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.01\n",
       "  },\n",
       "  \"id\": \"ft-PzwpGKewlzXC1fDppl3IAiin\",\n",
       "  \"model\": \"ada\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-rSvnkirO3SMFhS8NAtAozdxK\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"pending\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 28792529,\n",
       "      \"created_at\": 1684274350,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-mjKGzWqB6QJgbR337S2mQTgf\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1684274446,\n",
       "  \"validation_files\": [\n",
       "    {\n",
       "      \"bytes\": 5219225,\n",
       "      \"created_at\": 1684274410,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-wJevgYfAUGFx50S5xR4Y9Od8\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune model\n",
    "fine_tune_response = openai.FineTune.create(training_file=training_file_id, validation_file=valid_file_id, model=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTune fine-tune id=ft-PzwpGKewlzXC1fDppl3IAiin at 0x7fe4bb475180> JSON: {\n",
       "  \"created_at\": 1684274446,\n",
       "  \"events\": [\n",
       "    {\n",
       "      \"created_at\": 1684274446,\n",
       "      \"level\": \"info\",\n",
       "      \"message\": \"Created fine-tune: ft-PzwpGKewlzXC1fDppl3IAiin\",\n",
       "      \"object\": \"fine-tune-event\"\n",
       "    }\n",
       "  ],\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"hyperparams\": {\n",
       "    \"batch_size\": null,\n",
       "    \"learning_rate_multiplier\": null,\n",
       "    \"n_epochs\": 4,\n",
       "    \"prompt_loss_weight\": 0.01\n",
       "  },\n",
       "  \"id\": \"ft-PzwpGKewlzXC1fDppl3IAiin\",\n",
       "  \"model\": \"ada\",\n",
       "  \"object\": \"fine-tune\",\n",
       "  \"organization_id\": \"org-rSvnkirO3SMFhS8NAtAozdxK\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"pending\",\n",
       "  \"training_files\": [\n",
       "    {\n",
       "      \"bytes\": 28792529,\n",
       "      \"created_at\": 1684274350,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-mjKGzWqB6QJgbR337S2mQTgf\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ],\n",
       "  \"updated_at\": 1684274446,\n",
       "  \"validation_files\": [\n",
       "    {\n",
       "      \"bytes\": 5219225,\n",
       "      \"created_at\": 1684274410,\n",
       "      \"filename\": \"file\",\n",
       "      \"id\": \"file-wJevgYfAUGFx50S5xR4Y9Od8\",\n",
       "      \"object\": \"file\",\n",
       "      \"purpose\": \"fine-tune\",\n",
       "      \"status\": \"processed\",\n",
       "      \"status_details\": null\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tunning model with jobID: ft-PzwpGKewlzXC1fDppl3IAiin.\n",
      "Training Response: {\n",
      "  \"created_at\": 1684274446,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1684274446,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-PzwpGKewlzXC1fDppl3IAiin\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": null,\n",
      "    \"n_epochs\": 4,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-PzwpGKewlzXC1fDppl3IAiin\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-rSvnkirO3SMFhS8NAtAozdxK\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 28792529,\n",
      "      \"created_at\": 1684274350,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-mjKGzWqB6QJgbR337S2mQTgf\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1684274446,\n",
      "  \"validation_files\": [\n",
      "    {\n",
      "      \"bytes\": 5219225,\n",
      "      \"created_at\": 1684274410,\n",
      "      \"filename\": \"file\",\n",
      "      \"id\": \"file-wJevgYfAUGFx50S5xR4Y9Od8\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Training Status: pending\n"
     ]
    }
   ],
   "source": [
    "job_id = fine_tune_response[\"id\"]\n",
    "status = fine_tune_response[\"status\"]\n",
    "\n",
    "print(f'Fine-tunning model with jobID: {job_id}.')\n",
    "print(f\"Training Response: {fine_tune_response}\")\n",
    "print(f\"Training Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming events for the fine-tuning job: ft-PzwpGKewlzXC1fDppl3IAiin\n",
      "2023-05-17 00:00:46 Created fine-tune: ft-PzwpGKewlzXC1fDppl3IAiin\n",
      "2023-05-17 00:03:12 Fine-tune costs $10.10\n",
      "2023-05-17 00:03:12 Fine-tune enqueued. Queue number: 12\n",
      "2023-05-17 00:10:08 Fine-tune is in the queue. Queue number: 11\n",
      "2023-05-17 00:10:09 Fine-tune is in the queue. Queue number: 10\n",
      "2023-05-17 00:10:55 Fine-tune is in the queue. Queue number: 9\n",
      "2023-05-17 00:12:13 Fine-tune is in the queue. Queue number: 8\n",
      "2023-05-17 00:12:15 Fine-tune is in the queue. Queue number: 7\n",
      "2023-05-17 00:12:49 Fine-tune is in the queue. Queue number: 6\n",
      "2023-05-17 00:14:04 Fine-tune is in the queue. Queue number: 5\n",
      "2023-05-17 00:14:22 Fine-tune is in the queue. Queue number: 4\n",
      "2023-05-17 00:14:22 Fine-tune is in the queue. Queue number: 3\n",
      "2023-05-17 00:14:24 Fine-tune is in the queue. Queue number: 2\n",
      "2023-05-17 00:14:25 Fine-tune is in the queue. Queue number: 1\n",
      "2023-05-17 00:14:26 Fine-tune is in the queue. Queue number: 0\n",
      "2023-05-17 00:14:48 Fine-tune started\n",
      "2023-05-17 00:26:49 Completed epoch 1/4\n",
      "2023-05-17 00:50:12 Completed epoch 3/4\n",
      "2023-05-17 01:02:15 Uploaded model: ada:ft-personal-2023-05-16-23-02-15\n",
      "2023-05-17 01:02:17 Uploaded result file: file-YQBffVtwQBXa3tGBB1mPgzXi\n",
      "2023-05-17 01:02:17 Fine-tune succeeded\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "import datetime\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "\tstatus = openai.FineTune.retrieve(job_id).status\n",
    "\tprint(f\"Stream interrupted. Job is still {status}.\")\n",
    "\treturn\n",
    "\n",
    "# Check fine-tuning progress\n",
    "print(f'Streaming events for the fine-tuning job: {job_id}')\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "events = openai.FineTune.stream_events(job_id)\n",
    "try:\n",
    "    for event in events:\n",
    "    \tprint(f'{datetime.datetime.fromtimestamp(event[\"created_at\"])} {event[\"message\"]}')\n",
    "except Exception:\n",
    "\tprint(\"Stream interrupted (client disconnected).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune job ft-PzwpGKewlzXC1fDppl3IAiin finished with status: succeeded\n",
      "Checking other finetune jobs in the subscription.\n",
      "Found 1 finetune jobs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Confirm fine-tuning finished\n",
    "status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "\tprint(f'Job not in terminal status: {status}. Waiting.')\n",
    "\twhile status not in [\"succeeded\", \"failed\"]:\n",
    "\t\ttime.sleep(2)\n",
    "\t\tstatus = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "\t\tprint(f'Status: {status}')\n",
    "else:\n",
    "\tprint(f'Finetune job {job_id} finished with status: {status}')\n",
    "\n",
    "print('Checking other finetune jobs in the subscription.')\n",
    "result = openai.FineTune.list()\n",
    "print(f'Found {len(result.data)} finetune jobs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ada:ft-personal-2023-05-16-23-02-15'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_response = openai.FineTune.retrieve(job_id)\n",
    "fine_tuned_model = retrieve_response.fine_tuned_model\n",
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch model evalutaion statistics\n",
    "!openai api fine_tunes.results -i \"ft-PzwpGKewlzXC1fDppl3IAiin\" > real_training_stats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -184'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate fine-tuned model on new prompts\n",
    "new_prompt = '''entergy corporation and subsidiaries management 2019s financial discussion and analysis a result of the entergy louisiana and entergy gulf states louisiana business combination , results of operations for 2015 also include two items that occurred in october 2015 : 1 ) a deferred tax asset and resulting net increase in tax basis of approximately $ 334 million and 2 ) a regulatory liability of $ 107 million ( $ 66 million net-of-tax ) as a result of customer credits to be realized by electric customers of entergy louisiana , consistent with the terms of the stipulated settlement in the business combination proceeding .\n",
    "see note 2 to the financial statements for further discussion of the business combination and customer credits .\n",
    "results of operations for 2015 also include the sale in december 2015 of the 583 mw rhode island state energy center for a realized gain of $ 154 million ( $ 100 million net-of-tax ) on the sale and the $ 77 million ( $ 47 million net-of-tax ) write-off and regulatory charges to recognize that a portion of the assets associated with the waterford 3 replacement steam generator project is no longer probable of recovery .\n",
    "see note 14 to the financial statements for further discussion of the rhode island state energy center sale .\n",
    "see note 2 to the financial statements for further discussion of the waterford 3 write-off .\n",
    "results of operations for 2014 include $ 154 million ( $ 100 million net-of-tax ) of charges related to vermont yankee primarily resulting from the effects of an updated decommissioning cost study completed in the third quarter 2014 along with reassessment of the assumptions regarding the timing of decommissioning cash flows and severance and employee retention costs .\n",
    "see note 14 to the financial statements for further discussion of the charges .\n",
    "results of operations for 2014 also include the $ 56.2 million ( $ 36.7 million net-of-tax ) write-off in 2014 of entergy mississippi 2019s regulatory asset associated with new nuclear generation development costs as a result of a joint stipulation entered into with the mississippi public utilities staff , subsequently approved by the mpsc , in which entergy mississippi agreed not to pursue recovery of the costs deferred by an mpsc order in the new nuclear generation docket .\n",
    "see note 2 to the financial statements for further discussion of the new nuclear generation development costs and the joint stipulation .\n",
    "net revenue utility following is an analysis of the change in net revenue comparing 2015 to 2014 .\n",
    "amount ( in millions ) .\n",
    "the retail electric price variance is primarily due to : 2022 formula rate plan increases at entergy louisiana , as approved by the lpsc , effective december 2014 and january 2015 ; 2022 an increase in energy efficiency rider revenue primarily due to increases in the energy efficiency rider at entergy arkansas , as approved by the apsc , effective july 2015 and july 2014 , and new energy efficiency riders at entergy louisiana and entergy mississippi that began in the fourth quarter 2014 ; and 2022 an annual net rate increase at entergy mississippi of $ 16 million , effective february 2015 , as a result of the mpsc order in the june 2014 rate case .\n",
    "see note 2 to the financial statements for a discussion of rate and regulatory proceedings. .\n",
    "[[\"\", \"Amount (In Millions)\"], [\"2014 net revenue\", \"$5,735\"], [\"Retail electric price\", \"187\"], [\"Volume/weather\", \"95\"], [\"Waterford 3 replacement steam generator provision\", \"(32)\"], [\"MISO deferral\", \"(35)\"], [\"Louisiana business combination customer credits\", \"(107)\"], [\"Other\", \"(14)\"], [\"2015 net revenue\", \"$5,829\"]]\n",
    "[[\"\", \"amount ( in millions )\"], [\"2014 net revenue\", \"$ 5735\"], [\"retail electric price\", \"187\"], [\"volume/weather\", \"95\"], [\"waterford 3 replacement steam generator provision\", \"-32 ( 32 )\"], [\"miso deferral\", \"-35 ( 35 )\"], [\"louisiana business combination customer credits\", \"-107 ( 107 )\"], [\"other\", \"-14 ( 14 )\"], [\"2015 net revenue\", \"$ 5829\"]]\n",
    "what is the net change in net revenue during 2015 for entergy corporation?\\n\\n###\\n\\n'''\n",
    "\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=2,\n",
    "  temperature=0\n",
    ")\n",
    "answer['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 12%\\n\\n###'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt = '''item 1b .\n",
    "unresolved staff comments not applicable .\n",
    "item 2 .\n",
    "properties as of december 26 , 2015 , our major facilities consisted of : ( square feet in millions ) united states countries total owned facilities1 .\n",
    "30.7 17.2 47.9 leased facilities2 .\n",
    "2.1 6.0 8.1 .\n",
    "1 leases on portions of the land used for these facilities expire on varying dates through 2062 .\n",
    "2 leases expire on varying dates through 2030 and generally include renewals at our option .\n",
    "our principal executive offices are located in the u.s .\n",
    "and a majority of our wafer fabrication activities are also located in the u.s .\n",
    "we completed construction of development fabrication facilities in oregon during 2014 that we expect will enable us to maintain our process technology lead .\n",
    "we also completed construction of a large-scale fabrication building in arizona in 2013 .\n",
    "a portion of the new oregon and arizona facilities are currently not in use and we are reserving the new buildings for additional capacity and future technologies .\n",
    "incremental construction and equipment installation are required to ready the facilities for their intended use .\n",
    "our massachusetts fabrication facility was our last manufacturing facility on 200mm wafers and ceased production in q1 2015 .\n",
    "outside the u.s. , we have wafer fabrication facilities in ireland , israel , and china .\n",
    "our fabrication facility in ireland has transitioned to our 14nm process technology , with manufacturing continuing to ramp in 2016 .\n",
    "additionally , in the second half of 2016 , we will start using our facility in dalian , china to help expand our manufacturing capacity in next-generation memory .\n",
    "our assembly and test facilities are located in malaysia , china , and vietnam .\n",
    "in addition , we have sales and marketing offices worldwide that are generally located near major concentrations of customers .\n",
    "we believe that the facilities described above are suitable and adequate for our present purposes and that the productive capacity in our facilities is substantially being utilized or we have plans to utilize it .\n",
    "we do not identify or allocate assets by operating segment .\n",
    "for information on net property , plant and equipment by country , see 201cnote 26 : operating segments and geographic information 201d in part ii , item 8 of this form 10-k .\n",
    "item 3 .\n",
    "legal proceedings for a discussion of legal proceedings , see 201cnote 25 : contingencies 201d in part ii , item 8 of this form 10-k .\n",
    "item 4 .\n",
    "mine safety disclosures not applicable. .\n",
    "[[\"(Square Feet in Millions)\", \"UnitedStates\", \"OtherCountries\", \"Total\"], [\"Owned facilities<sup>1</sup>\", \"30.7\", \"17.2\", \"47.9\"], [\"Leased facilities<sup>2</sup>\", \"2.1\", \"6.0\", \"8.1\"], [\"Total facilities\", \"32.8\", \"23.2\", \"56.0\"]]\n",
    "[[\"( square feet in millions )\", \"unitedstates\", \"othercountries\", \"total\"], [\"owned facilities1\", \"30.7\", \"17.2\", \"47.9\"], [\"leased facilities2\", \"2.1\", \"6.0\", \"8.1\"], [\"total facilities\", \"32.8\", \"23.2\", \"56.0\"]]\n",
    "what percentage of total facilities as measured in square feet are leased?\\n\\n###\\n\\n'''\n",
    "\n",
    "answer = openai.Completion.create(\n",
    "  model=fine_tuned_model,\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=5,\n",
    "  temperature=0\n",
    ")\n",
    "answer['choices'][0]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
